# vLLM-Ascend éƒ¨ç½²é¡¹ç›®

åŸºäºåä¸ºæ˜‡è…¾Atlas 300I Duoçš„vLLMæ¨ç†æ¡†æ¶éƒ¨ç½²æ–¹æ¡ˆï¼Œæ”¯æŒQwen3æ¨¡å‹çš„å¿«/æ…¢æ€è€ƒåŒæ¨¡å¼ã€‚

## ğŸ“‹ é¡¹ç›®ä¿¡æ¯

- **ç¡¬ä»¶å¹³å°**: Atlas 300I Duo
- **èŠ¯ç‰‡æ¶æ„**: x86_64
- **CANNç‰ˆæœ¬**: 8.2.RC1
- **æ¨ç†æ¡†æ¶**: vLLM-Ascend
- **æ”¯æŒæ¨¡å‹**: Qwen3-0.6B (æ”¯æŒå¿«/æ…¢æ€è€ƒæ¨¡å¼)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å‰ç½®è¦æ±‚

1. å·²å®‰è£…æ˜‡è…¾é©±åŠ¨å’ŒCANN Toolkit 8.2.RC1
2. Dockerå·²å®‰è£…å¹¶é…ç½®
3. è‡³å°‘16GBå†…å­˜å’Œä¸€å—Ascend NPU

### ä¸€é”®éƒ¨ç½²
```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/lvhuaqing20/vllm-ascend-deployment.git
cd vllm-ascend-deployment

# æ„å»ºDockeré•œåƒ
./scripts/build.sh

# å¯åŠ¨æœåŠ¡ï¼ˆå¿«æ€è€ƒæ¨¡å¼ï¼‰
./scripts/run.sh fast

# æˆ–å¯åŠ¨æ…¢æ€è€ƒæ¨¡å¼
./scripts/run.sh slow
```

## ğŸ“ é¡¹ç›®ç»“æ„
```
vllm-ascend-deployment/
â”œâ”€â”€ README.md                 # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ LICENSE                   # å¼€æºè®¸å¯è¯
â”œâ”€â”€ .gitignore               # Gitå¿½ç•¥è§„åˆ™
â”œâ”€â”€ Dockerfile               # Dockeré•œåƒæ„å»ºæ–‡ä»¶
â”œâ”€â”€ requirements.txt         # Pythonä¾èµ–
â”œâ”€â”€ config/                  # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ fast_mode.yaml      # å¿«æ€è€ƒæ¨¡å¼é…ç½®
â”‚   â””â”€â”€ slow_mode.yaml      # æ…¢æ€è€ƒæ¨¡å¼é…ç½®
â”œâ”€â”€ scripts/                 # è„šæœ¬ç›®å½•
â”‚   â”œâ”€â”€ build.sh            # æ„å»ºè„šæœ¬
â”‚   â”œâ”€â”€ run.sh              # è¿è¡Œè„šæœ¬
â”‚   â”œâ”€â”€ test.sh             # æµ‹è¯•è„šæœ¬
â”‚   â””â”€â”€ setup_env.sh        # ç¯å¢ƒè®¾ç½®è„šæœ¬
â”œâ”€â”€ src/                     # æºä»£ç ç›®å½•
â”‚   â”œâ”€â”€ server.py           # æœåŠ¡å™¨ä¸»ç¨‹åº
â”‚   â””â”€â”€ utils.py            # å·¥å…·å‡½æ•°
â”œâ”€â”€ tests/                   # æµ‹è¯•ç›®å½•
â”‚   â”œâ”€â”€ test_api.py         # APIæµ‹è¯•
â”‚   â””â”€â”€ benchmark.py        # æ€§èƒ½æµ‹è¯•
â””â”€â”€ docs/                    # æ–‡æ¡£ç›®å½•
    â”œâ”€â”€ deployment.md       # éƒ¨ç½²æ–‡æ¡£
    â”œâ”€â”€ api.md              # APIæ–‡æ¡£
    â””â”€â”€ troubleshooting.md  # æ•…éšœæ’é™¤
```

## ğŸ¯ åŠŸèƒ½ç‰¹æ€§

### å¿«æ€è€ƒæ¨¡å¼ (Fast Mode)
- **é€‚ç”¨åœºæ™¯**: å¿«é€Ÿé—®ç­”ã€å®æ—¶å¯¹è¯
- **æœ€å¤§åºåˆ—é•¿åº¦**: 4096 tokens
- **å¹¶å‘è¯·æ±‚æ•°**: 64
- **æ¨ç†å»¶è¿Ÿ**: < 1s
- **Temperature**: 0.7-1.0

### æ…¢æ€è€ƒæ¨¡å¼ (Slow Mode)
- **é€‚ç”¨åœºæ™¯**: å¤æ‚æ¨ç†ã€é•¿æ–‡æœ¬ç”Ÿæˆ
- **æœ€å¤§åºåˆ—é•¿åº¦**: 8192 tokens
- **å¹¶å‘è¯·æ±‚æ•°**: 32
- **æ¨ç†å»¶è¿Ÿ**: 2-5s
- **Temperature**: 0.1-0.5

## ğŸ“Š æ€§èƒ½æŒ‡æ ‡

| æ¨¡å¼ | ååé‡ (req/s) | P50å»¶è¿Ÿ (ms) | P99å»¶è¿Ÿ (ms) |
|------|---------------|--------------|--------------|
| Fast | 45-60         | 800          | 1500         |
| Slow | 15-25         | 2500         | 4500         |

*æµ‹è¯•ç¯å¢ƒ: Atlas 300I Duo, Qwen3-0.6B, batch_size=1*

## ğŸ”§ é…ç½®è¯´æ˜

### å¿«æ€è€ƒæ¨¡å¼é…ç½® (config/fast_mode.yaml)
```yaml
model:
  name: "Qwen3-0.6B"
  path: "/models/qwen3-0.6b"
  
inference:
  max_model_len: 4096
  max_num_seqs: 64
  dtype: "bfloat16"
  gpu_memory_utilization: 0.85
  
generation:
  temperature: 0.7
  top_p: 0.9
  max_tokens: 256
```

### æ…¢æ€è€ƒæ¨¡å¼é…ç½® (config/slow_mode.yaml)
```yaml
model:
  name: "Qwen3-0.6B"
  path: "/models/qwen3-0.6b"
  
inference:
  max_model_len: 8192
  max_num_seqs: 32
  dtype: "bfloat16"
  gpu_memory_utilization: 0.90
  
generation:
  temperature: 0.3
  top_p: 0.95
  max_tokens: 1024
```

## ğŸ› ï¸ ç¯å¢ƒå®‰è£…

### æ–¹æ³•1: Dockeréƒ¨ç½²ï¼ˆæ¨èï¼‰
```bash
# æ„å»ºé•œåƒ
docker build -t vllm-ascend:v0.1 .

# è¿è¡Œå®¹å™¨ï¼ˆå¿«æ€è€ƒæ¨¡å¼ï¼‰
docker run -d \
  --name vllm-fast \
  --device=/dev/davinci0 \
  --device=/dev/davinci_manager \
  --device=/dev/devmm_svm \
  --device=/dev/hisi_hdc \
  -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
  -v $(pwd)/models:/models \
  -e THINKING_MODE=fast \
  -p 8000:8000 \
  vllm-ascend:v0.1
```

### æ–¹æ³•2: æœ¬åœ°å®‰è£…
```bash
# 1. å®‰è£…CANN Toolkit
wget https://ascend-repo.obs.cn-east-2.myhuaweicloud.com/CANN/8.2.RC1/Ascend-cann-toolkit_8.2.RC1_linux-x86_64.run
chmod +x Ascend-cann-toolkit_8.2.RC1_linux-x86_64.run
./Ascend-cann-toolkit_8.2.RC1_linux-x86_64.run --install

# 2. è®¾ç½®ç¯å¢ƒå˜é‡
source /usr/local/Ascend/ascend-toolkit/set_env.sh

# 3. åˆ›å»ºPythonç¯å¢ƒ
conda create -n vllm-ascend python=3.9
conda activate vllm-ascend

# 4. å®‰è£…PyTorchå’Œtorch_npu
pip install torch==2.1.0
pip install torch_npu==2.1.0.post3

# 5. å…‹éš†å¹¶å®‰è£…vllm-ascend
git clone https://github.com/PannenetsF/vllm.git -b ascend_develop
cd vllm
pip install -r requirements-npu.txt
VLLM_TARGET_DEVICE=npu python setup.py install

# 6. ä¸‹è½½æ¨¡å‹
mkdir -p models
cd models

# ä½¿ç”¨æ–°çš„ hf å‘½ä»¤
hf download Qwen/Qwen3-0.6B --local-dir qwen3-0.6b

# æˆ–ä½¿ç”¨ git cloneï¼ˆè¾ƒæ…¢ï¼‰
git clone https://huggingface.co/Qwen/Qwen3-0.6B qwen3-0.6b

# å›½å†…ç”¨æˆ·å¯ä»¥ä½¿ç”¨é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
hf download Qwen/Qwen3-0.6B --local-dir qwen3-0.6b

## ğŸ“¡ APIä½¿ç”¨

### å¯åŠ¨æœåŠ¡
```bash
# å¿«æ€è€ƒæ¨¡å¼
python -m vllm.entrypoints.openai.api_server \
  --model /models/qwen3-0.6b \
  --device npu \
  --host 0.0.0.0 \
  --port 8000 \
  --max-model-len 4096 \
  --max-num-seqs 64
```

### APIè°ƒç”¨ç¤ºä¾‹

#### Python
```python
import requests

url = "http://localhost:8000/v1/completions"

payload = {
    "model": "/models/qwen3-0.6b",
    "prompt": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "max_tokens": 100,
    "temperature": 0.7
}

response = requests.post(url, json=payload)
print(response.json()["choices"][0]["text"])
```

#### cURL
```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/models/qwen3-0.6b",
    "prompt": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

## ğŸ§ª æµ‹è¯•

### è¿è¡Œå•å…ƒæµ‹è¯•
```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
./scripts/test.sh

# æˆ–ä½¿ç”¨pytest
pytest tests/ -v
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•
```bash
# è¿è¡Œæ€§èƒ½æµ‹è¯•
python tests/benchmark.py --mode fast --requests 100 --concurrency 10

# å¯¹æ¯”ä¸¤ç§æ¨¡å¼
python tests/benchmark.py --mode fast --requests 100
python tests/benchmark.py --mode slow --requests 100
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. NPUè®¾å¤‡æœªè¯†åˆ«
```bash
# æ£€æŸ¥NPUè®¾å¤‡
npu-smi info

# å¦‚æœæœªè¯†åˆ«ï¼Œæ£€æŸ¥é©±åŠ¨
cat /usr/local/Ascend/driver/version.info

# è®¾ç½®ç¯å¢ƒå˜é‡
export ASCEND_DEVICE_ID=0
```

#### 2. CANNç‰ˆæœ¬ä¸åŒ¹é…

ç¡®ä¿ä»¥ä¸‹ç»„ä»¶ç‰ˆæœ¬ä¸€è‡´ï¼š
- CANN Toolkit: 8.2.RC1
- CANN Driver: 8.2.RC1
- torch_npu: 2.1.0.post3

#### 3. å†…å­˜ä¸è¶³ (OOM)
```bash
# å‡å°‘å¹¶å‘æ•°
--max-num-seqs 32

# å‡å°‘åºåˆ—é•¿åº¦
--max-model-len 2048

# é™ä½å†…å­˜ä½¿ç”¨ç‡
--gpu-memory-utilization 0.7
```

#### 4. Dockerå®¹å™¨æ— æ³•è®¿é—®NPU
```bash
# ç¡®ä¿æŒ‚è½½æ‰€æœ‰å¿…è¦çš„è®¾å¤‡
docker run \
  --device=/dev/davinci0 \
  --device=/dev/davinci_manager \
  --device=/dev/devmm_svm \
  --device=/dev/hisi_hdc \
  -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \
  ...
```

## ğŸ“š æ–‡æ¡£

- [éƒ¨ç½²æŒ‡å—](docs/deployment.md) - è¯¦ç»†çš„éƒ¨ç½²æ­¥éª¤
- [APIæ–‡æ¡£](docs/api.md) - å®Œæ•´çš„APIå‚è€ƒ
- [æ•…éšœæ’é™¤](docs/troubleshooting.md) - å¸¸è§é—®é¢˜è§£å†³

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ™ è‡´è°¢

- [vLLM](https://github.com/vllm-project/vllm) - åŸå§‹vLLMé¡¹ç›®
- [vLLM-Ascend](https://github.com/PannenetsF/vllm/tree/ascend_develop) - æ˜‡è…¾é€‚é…ç‰ˆæœ¬
- [Qwen](https://github.com/QwenLM/Qwen) - Qwenæ¨¡å‹
- [åä¸ºæ˜‡è…¾](https://www.hiascend.com/) - æ˜‡è…¾AIå¤„ç†å™¨

## ğŸ“§ è”ç³»æ–¹å¼

- ä½œè€…: lvhuaqing20
- é‚®ç®±: jianuolei662@gmail.com
- é¡¹ç›®é“¾æ¥: https://github.com/lvhuaqing20/vllm-ascend-deployment
